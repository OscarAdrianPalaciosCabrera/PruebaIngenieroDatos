{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9b05583-82a5-467e-88d7-ad8f5362414b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, to_timestamp, when, row_number,\n",
    "    from_utc_timestamp, expr\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "#A continuacion leeremos los datos desde la capa Bronze\n",
    "df_bronze = spark.read.table(\"workspace.bronze.hvfhs\")\n",
    "print(f\"Registros leídos desde Bronze: {df_bronze.count()}\")\n",
    "\n",
    "#Identificamos columnas de fechas y valores monetarios que se encuentran como string y que deben ser normalizados\n",
    "df_bronze.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "ca558264-33a6-4314-b257-df3b9167f78c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Seguidamente vamos a castear los flags a booleanos\n",
    "df_typed = (\n",
    "    df_bronze\n",
    "    .withColumn(\"shared_request_flag\", col(\"shared_request_flag\") == \"Y\")\n",
    "    .withColumn(\"shared_match_flag\", col(\"shared_match_flag\") == \"Y\")\n",
    "    .withColumn(\"access_a_ride_flag\", col(\"access_a_ride_flag\") == \"Y\")\n",
    "    .withColumn(\"wav_request_flag\", col(\"wav_request_flag\") == \"Y\")\n",
    "    .withColumn(\"wav_match_flag\", col(\"wav_match_flag\") == \"Y\")\n",
    ")\n",
    "\n",
    "#Validamos que los cambios se hayan realizado correctamente\n",
    "df_typed.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd3ce2a7-5312-4419-aaf8-f757fd64a6df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Ahora vamos a normalizar los timestamps que en crudo podemos ver que no tienen timezone\n",
    "from pyspark.sql.functions import to_utc_timestamp\n",
    "\n",
    "df_time = (\n",
    "    df_typed\n",
    "    .withColumn(\"pickup_datetime\", to_utc_timestamp(col(\"pickup_datetime\"), \"UTC\"))\n",
    "    .withColumn(\"dropoff_datetime\", to_utc_timestamp(col(\"dropoff_datetime\"), \"UTC\"))\n",
    "    .withColumn(\"request_datetime\", to_utc_timestamp(col(\"request_datetime\"), \"UTC\"))\n",
    "    .withColumn(\"on_scene_datetime\", to_utc_timestamp(col(\"on_scene_datetime\"), \"UTC\"))\n",
    ")\n",
    "\n",
    "#validamos que la zona horaria ahora se interprete en UTC\n",
    "df_time.printSchema()\n",
    "spark.sql(\"SET spark.sql.session.timeZone\").show()\n",
    "\n",
    "# Los timestamps originales eran timestamp_ntz.\n",
    "# Se normalizan a UTC para garantizar consistencia de los datos y evitar problemas de zona horaria\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5f7fe93-8dcb-4128-8e75-8595ba2c7497",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Pondremos el tiempo de viajes en minutos\n",
    "df_edited = (\n",
    "    df_time\n",
    "    .withColumn(\"trip_duration_minutes\", col(\"trip_time\") / 60)\n",
    ")\n",
    "\n",
    "#verificamos los valores originales vs los nuevos\n",
    "df_edited.select(\n",
    "    \"trip_time\",\n",
    "    \"trip_duration_minutes\"\n",
    ").limit(5).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72a68789-4d0e-4e27-a294-14f864f84a8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Por ultimo haremos algunas validaciones que pueden ser relevantes como tiempo de viaje valido, distacia de viaje mayor a 0 etc\n",
    "\n",
    "df_valid = (\n",
    "    df_edited\n",
    "    .filter(col(\"pickup_datetime\").isNotNull())\n",
    "    .filter(col(\"dropoff_datetime\") >= col(\"pickup_datetime\"))\n",
    "    .filter(col(\"trip_miles\") >= 0)\n",
    "    .filter(col(\"base_passenger_fare\") >= 0)\n",
    ")\n",
    "\n",
    "#verificamos la cantidad de registros antes y despues para corroborar que si funcionaron los filtros\n",
    "total_before = df_edited.count()\n",
    "total_after = df_valid.count()\n",
    "\n",
    "print(f\"Registros antes de calidad: {total_before}\")\n",
    "print(f\"Registros después de calidad: {total_after}\")\n",
    "print(f\"Registros filtrados: {total_before - total_after}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6837c9f-36c4-4b7a-a9f3-97cc92dfcc0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Ahora le indicamos las columnas que queremos persistir en la capa de silver\n",
    "df_silver_final = df_valid.select(\n",
    "    \"hvfhs_license_num\",\n",
    "    \"dispatching_base_num\",\n",
    "    \"originating_base_num\",\n",
    "\n",
    "    \"request_datetime\",\n",
    "    \"on_scene_datetime\",\n",
    "    \"pickup_datetime\",\n",
    "    \"dropoff_datetime\",\n",
    "\n",
    "    \"PULocationID\",\n",
    "    \"DOLocationID\",\n",
    "\n",
    "    \"trip_miles\",\n",
    "    \"trip_time\",\n",
    "    \"trip_duration_minutes\",\n",
    "\n",
    "    \"base_passenger_fare\",\n",
    "    \"tolls\",\n",
    "    \"bcf\",\n",
    "    \"sales_tax\",\n",
    "    \"congestion_surcharge\",\n",
    "    \"airport_fee\",\n",
    "    \"cbd_congestion_fee\",\n",
    "    \"tips\",\n",
    "    \"driver_pay\",\n",
    "\n",
    "    \"shared_request_flag\",\n",
    "    \"shared_match_flag\",\n",
    "    \"access_a_ride_flag\",\n",
    "    \"wav_request_flag\",\n",
    "    \"wav_match_flag\",\n",
    "\n",
    "    \"ingestion_timestamp\",\n",
    "    \"source_file\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41e5f2a3-df5b-42e9-8ffe-e1fe7f5d3dfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#podemos agregar una columna que facilitara consultas por fecha de viaje\n",
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "df_silver_final = df_silver_final.withColumn(\n",
    "    \"pickup_date\",\n",
    "    to_date(\"pickup_datetime\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33cdac20-6281-4135-9431-98c88d0d5da9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Finalmente vamos a persisistir los datos en la capa silver\n",
    "(\n",
    "    df_silver_final.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"append\")\n",
    "    #Aqui se puede poner una columna de particion que sera util para optimizar consultas\n",
    "    .partitionBy(\"pickup_date\")\n",
    "    .saveAsTable(\"workspace.silver.hvfhs\")\n",
    ")\n",
    "\n",
    "print(\"✔️ Datos guardados correctamente en la capa Silver\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e598e126-932f-4dcf-8366-dd0de8cfd277",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#verificamos con una consulta que los datos hayan persistido en la tabla asignada\n",
    "spark.sql(\"SELECT COUNT(*) FROM workspace.silver.hvfhs\").show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "silver_depuracion_hvfhs",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
